[
  {
    "model": "https://huggingface.co/mlc-ai/Llama-3.2-1B-Instruct-q4f32_1-MLC",
    "model_id": "Llama-3.2-1B-Instruct-q4f32_1-MLC",
    "model_lib": "https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_48/Llama-3.2-1B-Instruct-q4f32_1-ctx4k_cs1k-webgpu.wasm",
    "vram_required_MB": 1128.82,
    "low_resource_required": true,
    "overrides": {
      "context_window_size": 4096
    }
  },
  {
    "model": "https://huggingface.co/mlc-ai/Llama-3.2-1B-Instruct-q4f16_1-MLC",
    "model_id": "Llama-3.2-1B-Instruct-q4f16_1-MLC",
    "model_lib": "https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_48/Llama-3.2-1B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm",
    "vram_required_MB": 879.04,
    "low_resource_required": true,
    "overrides": {
      "context_window_size": 4096
    }
  },
  {
    "model": "https://huggingface.co/mlc-ai/Llama-3.2-1B-Instruct-q0f32-MLC",
    "model_id": "Llama-3.2-1B-Instruct-q0f32-MLC",
    "model_lib": "https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_48/Llama-3.2-1B-Instruct-q0f32-ctx4k_cs1k-webgpu.wasm",
    "vram_required_MB": 5106.26,
    "low_resource_required": true,
    "overrides": {
      "context_window_size": 4096
    }
  },
  {
    "model": "https://huggingface.co/mlc-ai/Llama-3.2-1B-Instruct-q0f16-MLC",
    "model_id": "Llama-3.2-1B-Instruct-q0f16-MLC",
    "model_lib": "https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_48/Llama-3.2-1B-Instruct-q0f16-ctx4k_cs1k-webgpu.wasm",
    "vram_required_MB": 2573.13,
    "low_resource_required": true,
    "overrides": {
      "context_window_size": 4096
    }
  },
  {
    "model": "https://huggingface.co/mlc-ai/Llama-3.2-3B-Instruct-q4f32_1-MLC",
    "model_id": "Llama-3.2-3B-Instruct-q4f32_1-MLC",
    "model_lib": "https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_48/Llama-3.2-3B-Instruct-q4f32_1-ctx4k_cs1k-webgpu.wasm",
    "vram_required_MB": 2951.51,
    "low_resource_required": true,
    "overrides": {
      "context_window_size": 4096
    }
  },
  {
    "model": "https://huggingface.co/mlc-ai/Llama-3.2-3B-Instruct-q4f16_1-MLC",
    "model_id": "Llama-3.2-3B-Instruct-q4f16_1-MLC",
    "model_lib": "https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_48/Llama-3.2-3B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm",
    "vram_required_MB": 2263.69,
    "low_resource_required": true,
    "overrides": {
      "context_window_size": 4096
    }
  },
  {
    "model": "https://huggingface.co/mlc-ai/DeepSeek-R1-Distill-Qwen-7B-q4f16_1-MLC",
    "model_id": "DeepSeek-R1-Distill-Qwen-7B-q4f16_1-MLC",
    "model_lib": "https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_48/Qwen2-7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm",
    "low_resource_required": false,
    "vram_required_MB": 5106.67,
    "overrides": {
      "context_window_size": 4096
    }
  },
  {
    "model": "https://huggingface.co/mlc-ai/DeepSeek-R1-Distill-Qwen-7B-q4f32_1-MLC",
    "model_id": "DeepSeek-R1-Distill-Qwen-7B-q4f32_1-MLC",
    "model_lib": "https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_48/Qwen2-7B-Instruct-q4f32_1-ctx4k_cs1k-webgpu.wasm",
    "low_resource_required": false,
    "vram_required_MB": 5900.09,
    "overrides": {
      "context_window_size": 4096
    }
  },
  {
    "model": "https://huggingface.co/mlc-ai/DeepSeek-R1-Distill-Llama-8B-q4f32_1-MLC",
    "model_id": "DeepSeek-R1-Distill-Llama-8B-q4f32_1-MLC",
    "model_lib": "https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_48/Llama-3_1-8B-Instruct-q4f32_1-ctx4k_cs1k-webgpu.wasm",
    "vram_required_MB": 6101.01,
    "low_resource_required": false,
    "overrides": {
      "context_window_size": 4096
    }
  },
  {
    "model": "https://huggingface.co/mlc-ai/DeepSeek-R1-Distill-Llama-8B-q4f16_1-MLC",
    "model_id": "DeepSeek-R1-Distill-Llama-8B-q4f16_1-MLC",
    "model_lib": "https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_48/Llama-3_1-8B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm",
    "vram_required_MB": 5001,
    "low_resource_required": false,
    "overrides": {
      "context_window_size": 4096
    }
  }
]
